{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install imbalanced-learn seaborn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset Formation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import ast\n",
    "import numpy as np\n",
    "from glob import glob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "def compute_features(group):\n",
    "    features = {}\n",
    "    # Number of events\n",
    "    \n",
    "    # Extract move events\n",
    "    move_events = group[group[\"event_type\"] == \"move\"]\n",
    "    features[\"num_moves\"] = len(move_events)\n",
    "    # Extract click events\n",
    "    click_events = group[group[\"event_type\"] == \"click\"]\n",
    "    features[\"num_clicks\"] = len(click_events)\n",
    "    # Extract scroll events\n",
    "    scroll_events = group[group[\"event_type\"] == \"scroll\"]\n",
    "    features[\"num_scrolls\"] = len(scroll_events)\n",
    "\n",
    "    # Click positions\n",
    "    if len(click_events) >= 1:\n",
    "        features[\"click_x_mean\"] = click_events[\"x\"].mean()\n",
    "        features[\"click_y_mean\"] = click_events[\"y\"].mean()\n",
    "    else:\n",
    "        features[\"click_x_mean\"] = 0\n",
    "        features[\"click_y_mean\"] = 0\n",
    "\n",
    "    # Compute movement distance and related features\n",
    "    if len(move_events) >= 2:\n",
    "        move_events = move_events.sort_values(\"time\")\n",
    "        x = move_events[\"x\"].values\n",
    "        y = move_events[\"y\"].values\n",
    "        t = move_events[\"time\"].values\n",
    "        dx = np.diff(x)\n",
    "        dy = np.diff(y)\n",
    "        dt = np.diff(t)\n",
    "\n",
    "        # Handle zeros in dt to prevent division by zero\n",
    "        valid = dt != 0\n",
    "        dx = dx[valid]\n",
    "        dy = dy[valid]\n",
    "        dt = dt[valid]\n",
    "        distances = np.sqrt(dx**2 + dy**2)\n",
    "        total_distance = np.sum(distances)\n",
    "        features[\"movement_distance\"] = total_distance\n",
    "\n",
    "        if len(distances) > 0:\n",
    "            # Velocity calculations\n",
    "            velocities = distances / dt\n",
    "            features[\"velocity_mean\"] = np.mean(velocities)\n",
    "            features[\"velocity_max\"] = np.max(velocities)\n",
    "            features[\"velocity_min\"] = np.min(velocities)\n",
    "            features[\"velocity_sd\"] = np.std(velocities)\n",
    "            features[\"velocity_x_mean\"] = np.mean(dx / dt)\n",
    "            features[\"velocity_y_mean\"] = np.mean(dy / dt)\n",
    "        else:\n",
    "            # No valid velocities\n",
    "            features.update({\n",
    "                \"velocity_mean\": 0,\n",
    "                \"velocity_max\": 0,\n",
    "                \"velocity_min\": 0,\n",
    "                \"velocity_sd\": 0,\n",
    "                \"velocity_x_mean\": 0,\n",
    "                \"velocity_y_mean\": 0,\n",
    "            })\n",
    "            velocities = np.array([])\n",
    "\n",
    "        # Acceleration calculations\n",
    "        if len(velocities) >= 2:\n",
    "            dv = np.diff(velocities)\n",
    "            dt_acc = dt[1:]  # Time intervals for acceleration\n",
    "            valid_acc = dt_acc != 0\n",
    "            dv = dv[valid_acc]\n",
    "            dt_acc = dt_acc[valid_acc]\n",
    "            if len(dv) > 0:\n",
    "                dv_dt = dv / dt_acc\n",
    "                features[\"acceleration_mean\"] = np.mean(dv_dt)\n",
    "                features[\"acceleration_max\"] = np.max(dv_dt)\n",
    "                features[\"acceleration_min\"] = np.min(dv_dt)\n",
    "                features[\"acceleration_sd\"] = np.std(dv_dt)\n",
    "            else:\n",
    "                features.update({\n",
    "                    \"acceleration_mean\": 0,\n",
    "                    \"acceleration_max\": 0,\n",
    "                    \"acceleration_min\": 0,\n",
    "                    \"acceleration_sd\": 0,\n",
    "                })\n",
    "                dv_dt = np.array([])\n",
    "        else:\n",
    "            features.update({\n",
    "                \"acceleration_mean\": 0,\n",
    "                \"acceleration_max\": 0,\n",
    "                \"acceleration_min\": 0,\n",
    "                \"acceleration_sd\": 0,\n",
    "            })\n",
    "            dv_dt = np.array([])\n",
    "\n",
    "        # Jerk calculations\n",
    "        if len(dv_dt) >= 2:\n",
    "            da = np.diff(dv_dt)\n",
    "            dt_jerk = dt_acc[1:]\n",
    "            valid_jerk = dt_jerk != 0\n",
    "            da = da[valid_jerk]\n",
    "            dt_jerk = dt_jerk[valid_jerk]\n",
    "            if len(da) > 0:\n",
    "                da_dt = da / dt_jerk\n",
    "                features[\"jerk_mean\"] = np.mean(da_dt)\n",
    "                features[\"jerk_sd\"] = np.std(da_dt)\n",
    "            else:\n",
    "                features.update({\n",
    "                    \"jerk_mean\": 0,\n",
    "                    \"jerk_sd\": 0,\n",
    "                })\n",
    "        else:\n",
    "            features.update({\n",
    "                \"jerk_mean\": 0,\n",
    "                \"jerk_sd\": 0,\n",
    "            })\n",
    "\n",
    "        # Angular velocity calculations\n",
    "        angles = np.arctan2(dy, dx)\n",
    "        d_angle = np.diff(angles)\n",
    "        d_angle = (d_angle + np.pi) % (2 * np.pi) - np.pi  # Normalize angles\n",
    "        dt_ang = dt[1:]\n",
    "        valid_ang = dt_ang != 0\n",
    "        d_angle = d_angle[valid_ang]\n",
    "        dt_ang = dt_ang[valid_ang]\n",
    "        if len(d_angle) > 0:\n",
    "            angular_velocity = d_angle / dt_ang\n",
    "            features[\"angular_velocity_mean\"] = np.mean(angular_velocity)\n",
    "            features[\"angular_velocity_sd\"] = np.std(angular_velocity)\n",
    "        else:\n",
    "            features.update({\n",
    "                \"angular_velocity_mean\": 0,\n",
    "                \"angular_velocity_sd\": 0,\n",
    "            })\n",
    "    else:\n",
    "        features.update({\n",
    "            \"movement_distance\": 0,\n",
    "            \"velocity_mean\": 0,\n",
    "            \"velocity_max\": 0,\n",
    "            \"velocity_min\": 0,\n",
    "            \"velocity_sd\": 0,\n",
    "            \"velocity_x_mean\": 0,\n",
    "            \"velocity_y_mean\": 0,\n",
    "            \"acceleration_mean\": 0,\n",
    "            \"acceleration_max\": 0,\n",
    "            \"acceleration_min\": 0,\n",
    "            \"acceleration_sd\": 0,\n",
    "            \"jerk_mean\": 0,\n",
    "            \"jerk_sd\": 0,\n",
    "            \"angular_velocity_mean\": 0,\n",
    "            \"angular_velocity_sd\": 0,\n",
    "        })\n",
    "\n",
    "    # Movement duration\n",
    "    if len(move_events) >= 1:\n",
    "        features[\"movement_duration\"] = move_events[\"time\"].max() - move_events[\"time\"].min()\n",
    "    else:\n",
    "        features[\"movement_duration\"] = 0\n",
    "\n",
    "    # Pause time (idle cursor time)\n",
    "    total_time = group[\"time\"].max() - group[\"time\"].min()\n",
    "    features[\"total_time\"] = total_time\n",
    "    features[\"pause_time\"] = total_time - features[\"movement_duration\"]\n",
    "\n",
    "    # Flips (directional changes)\n",
    "    if len(move_events) >= 2 and len(dx) >= 2:\n",
    "        features[\"flips_x\"] = np.sum(np.diff(np.sign(dx)) != 0)\n",
    "        features[\"flips_y\"] = np.sum(np.diff(np.sign(dy)) != 0)\n",
    "    else:\n",
    "        features[\"flips_x\"] = 0\n",
    "        features[\"flips_y\"] = 0\n",
    "\n",
    "    # Number of pauses (idle periods)\n",
    "    if len(move_events) >= 2 and len(dt) >= 1:\n",
    "        idle_threshold = 0.2  # Define a threshold for idle time\n",
    "        pauses = dt[dt > idle_threshold]\n",
    "        features[\"pause_count\"] = len(pauses)\n",
    "    else:\n",
    "        features[\"pause_count\"] = 0\n",
    "\n",
    "    # Hold time for clicks\n",
    "    if len(click_events) >= 1:\n",
    "        pressed_events = click_events[click_events[\"pressed\"] == True]\n",
    "        released_events = click_events[click_events[\"pressed\"] == False]\n",
    "        if len(pressed_events) == len(released_events):\n",
    "            hold_times = released_events[\"time\"].values - pressed_events[\"time\"].values\n",
    "            features[\"hold_time_mean\"] = np.mean(hold_times) if len(hold_times) > 0 else 0\n",
    "            features[\"hold_time_sd\"] = np.std(hold_times) if len(hold_times) > 1 else 0\n",
    "        else:\n",
    "            features[\"hold_time_mean\"] = 0\n",
    "            features[\"hold_time_sd\"] = 0\n",
    "    else:\n",
    "        features[\"hold_time_mean\"] = 0\n",
    "        features[\"hold_time_sd\"] = 0\n",
    "\n",
    "    return pd.Series(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features saved to recordings/web_browsing_mouse_events_HJ_features.csv\n",
      "Features saved to recordings/web_browsing_mouse_events_jungwoo_features.csv\n",
      "Features saved to recordings/youtube_mouse_events_0_features.csv\n",
      "Features saved to recordings/chess_mouse_events_HJ_features.csv\n",
      "Features saved to recordings/web_browsing_mouse_events_0_features.csv\n",
      "Features saved to recordings/finding_mines_mouse_events_jungwoo_features.csv\n",
      "Features saved to recordings/youtube_mouse_events_jungwoo_features.csv\n",
      "Features saved to recordings/reading_mouse_events_HJ_features.csv\n",
      "Features saved to recordings/web_browsing_mouse_events_woohyun_features.csv\n",
      "Features saved to recordings/chatting_mouse_events_HJ_features.csv\n",
      "Features saved to recordings/reading_thesis_mouse_events_woohyun_features.csv\n",
      "Features saved to recordings/youtube_mouse_events_HJ_features.csv\n",
      "Features saved to recordings/finding_mines_mouse_events_HJ_features.csv\n",
      "Features saved to recordings/ppt_mouse_events_HJ_features.csv\n",
      "Features saved to recordings/ppt_mouse_events_jungwoo_features.csv\n",
      "Features saved to recordings/chess_mouse_events_jungwoo_features.csv\n"
     ]
    }
   ],
   "source": [
    "files = glob('recordings/*.txt')\n",
    "\n",
    "for file in files:\n",
    "    events = []\n",
    "    monitors = []\n",
    "    with open(file, 'r') as f:\n",
    "        monitors.append(ast.literal_eval(f.readline().strip()))\n",
    "        for line in f:\n",
    "            event = ast.literal_eval(line.strip())\n",
    "            events.append(event)\n",
    "\n",
    "    # Convert events to a list of dictionaries\n",
    "    width, height = monitors[0][1], monitors[0][2]\n",
    "    event_list = []\n",
    "    for event in events:\n",
    "        if event[0] == 'move':\n",
    "            event_dict = {'event_type': 'move', 'time': event[1], 'x': event[2]/width, 'y': event[3]/height}\n",
    "        elif event[0] == 'click':\n",
    "            event_dict = {'event_type': 'click', 'time': event[1], 'x': event[2]/width, 'y': event[3]/height,\n",
    "                        'button': event[4], 'pressed': event[5]}\n",
    "        elif event[0] == 'scroll':\n",
    "            event_dict = {'event_type': 'scroll', 'time': event[1], 'x': event[2]/width, 'y': event[3]/height,\n",
    "                        'dx': event[4], 'dy': event[5]}\n",
    "        else:\n",
    "            continue\n",
    "        event_list.append(event_dict)\n",
    "\n",
    "    # Convert to DataFrame and sort by time\n",
    "    df = pd.DataFrame(event_list)\n",
    "    df = df.sort_values('time').reset_index(drop=True)\n",
    "\n",
    "    # Step 2: Organize events into time windows\n",
    "    start_time = df['time'].min()\n",
    "    df['timestamp'] = ((df['time'] - start_time) // 0.1).astype(int)\n",
    "    \n",
    "    unique_timestamps = df['timestamp'].unique()\n",
    "    blank_timestamps = [i for i in range(unique_timestamps.min(), unique_timestamps.max() + 1) if i not in unique_timestamps]\n",
    "    \n",
    "    # insert blank rows for missing timestamps\n",
    "    new_rows = []\n",
    "    for ts in blank_timestamps:\n",
    "        new_rows.append({'timestamp': ts})\n",
    "    df = pd.concat([df, pd.DataFrame(new_rows)], ignore_index=True)\n",
    "    df = df.sort_values('timestamp').reset_index(drop=True).fillna(0)\n",
    "    \n",
    "\n",
    "    # Step 3: Extract features for each time window\n",
    "    # Apply the feature extraction function to each time window\n",
    "    features_df = df.groupby('timestamp').apply(compute_features, include_groups=False).reset_index(drop=True)\n",
    "    \n",
    "\n",
    "    features_df.to_csv(file.replace('.txt', '_features.csv').replace(\"recordings\", \"train_data\"), index=False)\n",
    "    print(f\"Features saved to {file.replace('.txt', '_features.csv')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "classification_classes = {\n",
    "  \"web_browsing\": 0, # web\n",
    "  # \"ppt\": 1, # design tools,\n",
    "  # \"reading\": 2, # reading papers\n",
    "  \"finding_mines\": 3, # game\n",
    "  \"chess\": 3, # game\n",
    "  \"youtube\": 4, # youtube\n",
    "  # \"chatting\": 5, # chatting\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics import f1_score, accuracy_score\n",
    "from sklearn.ensemble import GradientBoostingClassifier, RandomForestClassifier\n",
    "from sklearn.preprocessing import StandardScaler, PolynomialFeatures\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "\n",
    "def process_class_data(file, window_size):\n",
    "    df = pd.read_csv(file)\n",
    "    df = df.ffill().bfill()\n",
    "    df = df.dropna()\n",
    "    # Extract windows\n",
    "    windows = []\n",
    "    num_rows = df.shape[0]\n",
    "    window_interval = max(window_size // 2, 1)\n",
    "    for i in range(0, num_rows - window_size + 1, window_interval):\n",
    "        window = df.iloc[i:i+window_size]\n",
    "        # Extract features\n",
    "        features = window.values.flatten()\n",
    "        windows.append(features)\n",
    "    return windows\n",
    "\n",
    "def prepare_data(window_size=50):\n",
    "    # Process the data for each class\n",
    "    X_train = []\n",
    "    X_test = []\n",
    "    y_train = []\n",
    "    y_test = []\n",
    "    for tag, label in classification_classes.items():\n",
    "        files = glob(f'train_data/{tag}_*.csv')\n",
    "        for file in files:\n",
    "            # print(f'Processing {file} for class {tag}')\n",
    "            windows = process_class_data(file, window_size)\n",
    "            index = int(0.8 * len(windows))\n",
    "            X_train.extend(windows[:index])\n",
    "            X_test.extend(windows[index:])\n",
    "            y_train.extend([label] * index)\n",
    "            y_test.extend([label] * (len(windows) - index))\n",
    "\n",
    "    X_train = np.array(X_train)\n",
    "    X_test = np.array(X_test)\n",
    "    y_train = np.array(y_train)\n",
    "    y_test = np.array(y_test)\n",
    "\n",
    "    X_train = np.nan_to_num(X_train, posinf=np.finfo(np.float32).max, neginf=np.finfo(np.float32).min)\n",
    "    X_test = np.nan_to_num(X_test, posinf=np.finfo(np.float32).max, neginf=np.finfo(np.float32).min)\n",
    "\n",
    "    # Normalize the data\n",
    "    scaler = StandardScaler()\n",
    "    X_train = scaler.fit_transform(X_train)\n",
    "    X_test = scaler.transform(X_test)\n",
    "\n",
    "    # Print per-class counts\n",
    "    # for label, count in zip(*np.unique(y_train, return_counts=True)):\n",
    "    #     print(f'Class {label}: {count} samples')\n",
    "\n",
    "    oversampler = RandomOverSampler(random_state=42)\n",
    "    X_resampled, y_resampled = oversampler.fit_resample(X_train, y_train)\n",
    "\n",
    "    # print(f'X_train shape: {X_train.shape} -> {X_resampled.shape}')\n",
    "    # print(f'X_test shape: {X_test.shape}')\n",
    "    return X_resampled, X_test, y_resampled, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'glob' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m X_resampled, X_test, y_resampled, y_test \u001b[38;5;241m=\u001b[39m \u001b[43mprepare_data\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      2\u001b[0m X_resampled\u001b[38;5;241m.\u001b[39mshape, X_test\u001b[38;5;241m.\u001b[39mshape, y_resampled\u001b[38;5;241m.\u001b[39mshape, y_test\u001b[38;5;241m.\u001b[39mshape\n",
      "Cell \u001b[0;32mIn[2], line 34\u001b[0m, in \u001b[0;36mprepare_data\u001b[0;34m(window_size)\u001b[0m\n\u001b[1;32m     32\u001b[0m y_test \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m     33\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m tag, label \u001b[38;5;129;01min\u001b[39;00m classification_classes\u001b[38;5;241m.\u001b[39mitems():\n\u001b[0;32m---> 34\u001b[0m     files \u001b[38;5;241m=\u001b[39m \u001b[43mglob\u001b[49m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrain_data/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtag\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_*.csv\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     35\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m file \u001b[38;5;129;01min\u001b[39;00m files:\n\u001b[1;32m     36\u001b[0m         \u001b[38;5;66;03m# print(f'Processing {file} for class {tag}')\u001b[39;00m\n\u001b[1;32m     37\u001b[0m         windows \u001b[38;5;241m=\u001b[39m process_class_data(file, window_size)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'glob' is not defined"
     ]
    }
   ],
   "source": [
    "X_resampled, X_test, y_resampled, y_test = prepare_data(1)\n",
    "X_resampled.shape, X_test.shape, y_resampled.shape, y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "def train(model_name, window_size):\n",
    "    if model_name == 'boosting':\n",
    "        model = GradientBoostingClassifier(random_state=42, verbose=1)\n",
    "    elif model_name == 'random_forest':\n",
    "        model = RandomForestClassifier(random_state=42)\n",
    "    elif model_name == 'svm':\n",
    "        model = SVC(random_state=42)\n",
    "    elif model_name == 'logistic_regression':\n",
    "        model = LogisticRegression(tol=1e-3)\n",
    "    elif model_name == 'poly_regression':\n",
    "        model = make_pipeline(PolynomialFeatures(degree=2), LogisticRegression())\n",
    "    elif model_name == 'knn':\n",
    "        model = KNeighborsClassifier()\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown MODEL: {model_name}\")\n",
    "\n",
    "    X_resampled, X_test, y_resampled, y_test = prepare_data(window_size)\n",
    "    \n",
    "    model.fit(X_resampled, y_resampled)\n",
    "\n",
    "    # Predict on the test set and evaluate performance\n",
    "    y_pred = model.predict(X_test)\n",
    "    f1 = f1_score(y_test, y_pred, average='weighted')\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    # return conf matrix\n",
    "    cm = confusion_matrix(y_test, y_pred)\n",
    "    \n",
    "    return accuracy, f1, cm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'train' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 15\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m model_name \u001b[38;5;129;01min\u001b[39;00m [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mboosting\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrandom_forest\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlogistic_regression\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mknn\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpoly_regression\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124msvm\u001b[39m\u001b[38;5;124m'\u001b[39m]: \n\u001b[1;32m     14\u001b[0m   \u001b[38;5;28;01mfor\u001b[39;00m window_size \u001b[38;5;129;01min\u001b[39;00m [\u001b[38;5;241m50\u001b[39m, \u001b[38;5;241m100\u001b[39m, \u001b[38;5;241m600\u001b[39m]:\n\u001b[0;32m---> 15\u001b[0m     acc, f1, cm \u001b[38;5;241m=\u001b[39m \u001b[43mtrain\u001b[49m(model_name, window_size)\n\u001b[1;32m     16\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mModel: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodel_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, Window Size: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mwindow_size\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, Accuracy: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00macc\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, F1: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mf1\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     17\u001b[0m     \u001b[38;5;66;03m# plot cm\u001b[39;00m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'train' is not defined"
     ]
    }
   ],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "category_names = []\n",
    "for label in range(6):\n",
    "  name = \"\"\n",
    "  for tag, l in classification_classes.items():\n",
    "    if l == label:\n",
    "      name += tag + \"/\"\n",
    "  category_names.append(name)\n",
    "\n",
    "# Options: 'boosting', 'random_forest', 'svm', 'logistic_regression', 'poly_regression', 'knn'\n",
    "for model_name in ['boosting', 'random_forest', 'logistic_regression', 'knn', 'poly_regression', 'svm']: \n",
    "  for window_size in [50, 100, 600]:\n",
    "    acc, f1, cm = train(model_name, window_size)\n",
    "    print(f\"Model: {model_name}, Window Size: {window_size}, Accuracy: {acc}, F1: {f1}\")\n",
    "    # plot cm\n",
    "    sns.heatmap(cm, annot=True, fmt='g', xticklabels=category_names, yticklabels=category_names)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
